{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression In TensorFlow\n",
    "We can't talk about linear regression without logistic regression. Let's illustrate logistic regression in TensorFlow solving the good old classifier on the MNIST database.\n",
    "The MNIST database (Mixed National Institute of Standards and Technology database) is probably one of the most popular databases used for training various image processing systems it is a database of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28 pixels, flatten to be a 1-d tensor of size 784. Each comes with a label. For example, images on the first row is labelled as 0, the second as 1, and so on. The dataset in hosted on Yann Lecun's website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: \n",
    "X = tf.placeholder(tf.float32, [batch_size, 784])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4:\n",
    "w = tf.Variable(tf.random_normal(shape=[784,10], stddev=0.01), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1,10]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5:\n",
    "logits = tf.matmul(X,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels= Y,logits = logits)\n",
    "#compute the mean over examples in the batch\n",
    "loss = tf.reduce_mean(entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: define Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "print n_batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9186\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([optimizer, loss], feed_dict={X:X_batch, Y:Y_batch})\n",
    "            \n",
    "    test_n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds= 0\n",
    "    for i in range(test_n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        opt_batch, loss_batch,logits_batch =  sess.run([optimizer, loss, logits],\n",
    "feed_dict={X: X_batch, Y:Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(Y_batch,1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    \n",
    "    print \"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
